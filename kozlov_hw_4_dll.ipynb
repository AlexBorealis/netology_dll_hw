{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504629d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(1111)\n",
    "random.seed(1111)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bac7c",
   "metadata": {},
   "source": [
    "# **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e806f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt(text, shift, alphabet):\n",
    "    new_alphabet = alphabet[shift % len(alphabet):] + alphabet[:shift % len(alphabet)]\n",
    "    result = ''\n",
    "    for char in text:\n",
    "        if char in alphabet:\n",
    "            result += new_alphabet[alphabet.index(char)]\n",
    "        else:\n",
    "            result += char\n",
    "    return result\n",
    "\n",
    "def decrypt(text, shift, alphabet):\n",
    "    return encrypt(text, -shift, alphabet)\n",
    "\n",
    "def generate_data(alphabet, max_len=10, shift=3):\n",
    "    length = random.randint(5, max_len)\n",
    "    text = ''.join(random.choice(alphabet) for _ in range(length))\n",
    "    encrypted = encrypt(text, shift, alphabet=alphabet)\n",
    "    return {'text': text, 'encrypted': encrypted, 'length': length}\n",
    "\n",
    "def text_to_one_hot(text, alphabet):\n",
    "    seq_len = len(text)\n",
    "    one_hot = np.zeros((seq_len, len(alphabet)), dtype=np.float32)\n",
    "    for i, char in enumerate(text):\n",
    "        if char in alphabet:\n",
    "            one_hot[i, alphabet.index(char)] = 1\n",
    "    return torch.tensor(one_hot)\n",
    "\n",
    "def dataset_to_tensors(dataset, alphabet):\n",
    "    max_len = max(pair['length'] for pair in dataset)\n",
    "    X = torch.zeros((len(dataset), max_len, len(alphabet)), dtype=torch.float32)\n",
    "    y = torch.zeros((len(dataset), max_len, len(alphabet)), dtype=torch.float32)\n",
    "    \n",
    "    for i, pair in enumerate(dataset):\n",
    "        X[i, :len(pair['encrypted'])] = text_to_one_hot(pair['encrypted'], alphabet)\n",
    "        y[i, :len(pair['text'])] = text_to_one_hot(pair['text'], alphabet)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "class CaesarNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CaesarNet, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        o, _ = self.rnn(sentence)\n",
    "        return self.linear(o)\n",
    "\n",
    "    def train_model(self, train_loader, alphabet_size, lr, epochs=10):\n",
    "        self.to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        start = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            train_loss, train_acc, train_iter_num = 0., 0., 0.\n",
    "            \n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = self(X_batch)\n",
    "                y_indices = torch.argmax(y_batch, dim=-1)\n",
    "                \n",
    "                loss = criterion(output.view(-1, alphabet_size), y_indices.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                batch_acc = (output.argmax(dim=-1) == y_indices).float().mean().item()\n",
    "                train_acc += batch_acc\n",
    "                train_iter_num += 1\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1}, loss: {train_loss/train_iter_num:.4f}, acc: \"\n",
    "                f\"{train_acc/train_iter_num:.4f}, \"\n",
    "                f\"{time.time() - start:.2f} sec.\"\n",
    "            )\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, text, alphabet):\n",
    "        self.eval()\n",
    "        best_shift = 0\n",
    "        best_score = -1.\n",
    "        best_pred_text = \"\"\n",
    "        \n",
    "        for shift in range(len(alphabet)):\n",
    "            encrypted = encrypt(text, shift, alphabet)\n",
    "            X_test = text_to_one_hot(encrypted, alphabet)[None, :]\n",
    "            with torch.no_grad():\n",
    "                X_test = X_test.to(device)\n",
    "                pred = self(X_test)\n",
    "            pred_text = ''.join(alphabet[torch.argmax(char).item()] for char in pred[0][:len(text)])\n",
    "            \n",
    "            matches = sum(1 for c1, c2 in zip(text, pred_text) if c1 == c2)\n",
    "            score = matches / len(text) if len(text) > 0 else 0.\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_shift = shift\n",
    "                best_pred_text = pred_text\n",
    "        \n",
    "        encrypted = encrypt(text, best_shift, alphabet)\n",
    "        return encrypted, best_pred_text, best_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56779ce6",
   "metadata": {},
   "source": [
    "# **Creation Alphabet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d7702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz' + 'abcdefghijklmnopqrstuvwxyz'.upper() + 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя' + 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'.upper() + r'0123456789,. ?|/\\[]()\";:'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86d45f",
   "metadata": {},
   "source": [
    "# **Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3191ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_size = len(alphabet)\n",
    "input_size = output_size = alphabet_size\n",
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "learning_rate = .001\n",
    "num_epochs = 10\n",
    "shift = random.randint(-3555456654575, 3555456654575) % alphabet_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38026235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cdf9e4",
   "metadata": {},
   "source": [
    "# **Creation initial dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df9456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_init = [generate_data(alphabet, max_len=250, shift=shift) for _ in range(10000)]\n",
    "X, y = dataset_to_tensors(dataset_init, alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69a26395",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, y)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01b25e",
   "metadata": {},
   "source": [
    "# **Creation Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb670663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaesarNet(\n",
       "  (rnn): RNN(142, 128, batch_first=True)\n",
       "  (linear): Linear(in_features=128, out_features=142, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CaesarNet(input_size, hidden_size, output_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df106771",
   "metadata": {},
   "source": [
    "# **Training model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "418cc9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, loss: 0.0203, acc: 1.0000, 22.08 sec.\n"
     ]
    }
   ],
   "source": [
    "model = model.train_model(train_loader, alphabet_size, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50743b76",
   "metadata": {},
   "source": [
    "# **Experiment 1: with bruteforce shift**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864938ac",
   "metadata": {},
   "source": [
    "## **Testing Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae5062be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Привет мир, это я DJ 23\n",
      "Encrypted: пKCvyMЛGCKЙЛXMIЛZЛ)cЛВГ\n",
      "Predicted: Привет мир, это я DJ 23\n",
      "Learning Shift: 109\n",
      "Initial Shift: 109\n"
     ]
    }
   ],
   "source": [
    "text = \"Привет мир, это я DJ 23\"\n",
    "encrypted, predicted, learning_shift = model.predict(text, alphabet)\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Encrypted: {encrypted}\")\n",
    "print(f\"Predicted: {predicted}\")\n",
    "print(f\"Learning Shift: {learning_shift}\")\n",
    "print(f\"Initial Shift: {shift}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3efc3451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: nN/wHПЖrЖYyьRhТАIgсбЖиАVmЫLЩщРf9ZASc6Ц9zxЙcз\"Ж1пщаИu]чЗкelf:ар1ШATEсКR1Ьуk:]uЬгЯ3s|yrц9еNAШ мкЕ7зюeОЮЖ ИокгщTw]бlyьЧл42ЩиLYkеЙ3QS5V3пШ/гтDwRjLДя\"YЛЙыУlжrgуцФчz9ашБmрHETrРзБ)жOdйrd:|CйfЙoZ)БааRHrшJ9CdOyАэдaэщ\n",
      "Encrypted: 4gО?aпж8жr/WkЮтаbЭLuжCаo3ыeщTрЬИs[lЩЁцИ\\|йЩBФжБJTtи.СRзEЫ2ЬЦtKБш[m\"LкkБьN1ЦС.ьwяГ9Н/8QИyg[шЛGEеЖBYЫоюжЛиIEwTm?Сu2/WчFДВщCer1yйГjlЕoГJшОwM)?k0eдZФrлйVу2A8ЭNQфR\\ИtSб3Ka\"m8рBбУAhЪD8ЪЦН(DЬй5sУбttka8ScИ(Ъh/аXxЧXT\n",
      "Predicted: nN/wHПЖrЖYyьRhТАIgсбЖиАVmЫLЩщРf9ZASc6Ц9zxЙcз\"Ж1пщаИu]чЗкelf:ар1ШATEсКR1Ьуk:]uЬгЯ3s|yrц9еNAШ мкЕ7зюeОЮЖ ИокгщTw]бlyьЧл42ЩиLYkеЙ3QS5V3пШ/гтDwRjLДя\"YЛЙыУlжrgуцФчz9ашБmрHETrРзБ)жOdйrd:|CйfЙoZ)БааRHrшJ9CdOyАэдaэщ\n",
      "Learning Shift: 109\n",
      "Initial Shift: 109\n"
     ]
    }
   ],
   "source": [
    "text = dataset_init[9999]['text']\n",
    "encrypted, predicted, learning_shift = model.predict(text, alphabet)\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Encrypted: {encrypted}\")\n",
    "print(f\"Predicted: {predicted}\")\n",
    "print(f\"Learning Shift: {learning_shift}\")\n",
    "print(f\"Initial Shift: {shift}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a4190",
   "metadata": {},
   "source": [
    "# **Experiment 2: with trainable shift**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce05d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaesarNetTrainShift(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CaesarNetTrainShift, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.shift = nn.Parameter(torch.tensor(float(random.randint(0, input_size - 1))))\n",
    "        self.alphabet_size = input_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        shift = torch.round(self.shift).long() % self.alphabet_size\n",
    "        shifted_sentence = torch.roll(sentence, shifts=shift.item(), dims=-1)\n",
    "        o, _ = self.rnn(shifted_sentence)\n",
    "        return self.linear(o)\n",
    "\n",
    "    def train_model(self, train_loader, alphabet_size, lr, epochs=10):\n",
    "        self.to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam([\n",
    "        {'params': [self.shift], 'lr': .01},\n",
    "        {'params': [p for p in self.parameters() if p is not self.shift], 'lr': lr}\n",
    "    ], weight_decay=1e-4)\n",
    "        start = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            train_loss, train_acc, train_iter_num = 0., 0., 0.\n",
    "            \n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = self(X_batch)\n",
    "                y_indices = torch.argmax(y_batch, dim=-1)\n",
    "                \n",
    "                loss = criterion(output.view(-1, alphabet_size), y_indices.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                batch_acc = (output.argmax(dim=-1) == y_indices).float().mean().item()\n",
    "                train_acc += batch_acc\n",
    "                train_iter_num += 1\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1}, loss: {train_loss/train_iter_num:.4f}, acc: \"\n",
    "                f\"{train_acc/train_iter_num:.4f},\"\n",
    "                f\"{time.time() - start:.2f} sec.\"\n",
    "            )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, text, alphabet, level=1/2):\n",
    "        self.eval()\n",
    "\n",
    "        learning_shift = int(round(self.shift.item())) % len(alphabet)\n",
    "        best_shift = 0\n",
    "        best_score = -1.\n",
    "        best_pred_text = \"\"\n",
    "        \n",
    "        for shift in range(learning_shift, len(alphabet)):\n",
    "            encrypted = encrypt(text, shift, alphabet)\n",
    "            X_test = text_to_one_hot(encrypted, alphabet)[None, :]\n",
    "            with torch.no_grad():\n",
    "                X_test = X_test.to(device)\n",
    "                pred = self(X_test)\n",
    "            pred_text = ''.join(alphabet[torch.argmax(char).item()] for char in pred[0][:len(text)])\n",
    "            \n",
    "            matches = sum(1 for c1, c2 in zip(text, pred_text) if c1 == c2)\n",
    "            score = matches / len(text) if len(text) > 0 else 0.\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_shift = shift\n",
    "                best_pred_text = pred_text\n",
    "            \n",
    "            if score > level:\n",
    "                break\n",
    "        \n",
    "        encrypted = encrypt(text, best_shift, alphabet)\n",
    "        return encrypted, best_pred_text, best_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aacd7d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaesarNetTrainShift(\n",
       "  (rnn): RNN(142, 128, batch_first=True)\n",
       "  (linear): Linear(in_features=128, out_features=142, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = CaesarNetTrainShift(input_size, hidden_size, output_size)\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6723fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, loss: 0.0204, acc: 1.0000,27.38 sec.\n"
     ]
    }
   ],
   "source": [
    "model1 = model1.train_model(train_loader, alphabet_size, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1866824a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(61., device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b53f5040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Привет мир, это я DJ 23\n",
      "Encrypted: пKCvyMЛGCKЙЛXMIЛZЛ)cЛВГ\n",
      "Predicted: Привет мир, это я DJ 23\n",
      "Learning Shift: 109\n",
      "Initial Shift: 109\n"
     ]
    }
   ],
   "source": [
    "text = \"Привет мир, это я DJ 23\"\n",
    "encrypted, predicted, learning_shift = model1.predict(text, alphabet)\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Encrypted: {encrypted}\")\n",
    "print(f\"Predicted: {predicted}\")\n",
    "print(f\"Learning Shift: {learning_shift}\")\n",
    "print(f\"Initial Shift: {shift}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb305ae0",
   "metadata": {},
   "source": [
    "# **Simpsons Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b1403",
   "metadata": {},
   "source": [
    "## **Creation dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88930b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                      int64\n",
      "episode_id              int64\n",
      "number                  int64\n",
      "raw_text               object\n",
      "timestamp_in_ms        object\n",
      "speaking_line          object\n",
      "character_id           object\n",
      "location_id           float64\n",
      "raw_character_text     object\n",
      "raw_location_text      object\n",
      "spoken_words           object\n",
      "normalized_text        object\n",
      "word_count             object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9549</td>\n",
       "      <td>32</td>\n",
       "      <td>209</td>\n",
       "      <td>Miss Hoover: No, actually, it was a little of ...</td>\n",
       "      <td>848000</td>\n",
       "      <td>true</td>\n",
       "      <td>464</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "      <td>no actually it was a little of both sometimes ...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9550</td>\n",
       "      <td>32</td>\n",
       "      <td>210</td>\n",
       "      <td>Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?</td>\n",
       "      <td>856000</td>\n",
       "      <td>true</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "      <td>wheres mr bergstrom</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9551</td>\n",
       "      <td>32</td>\n",
       "      <td>211</td>\n",
       "      <td>Miss Hoover: I don't know. Although I'd sure l...</td>\n",
       "      <td>856000</td>\n",
       "      <td>true</td>\n",
       "      <td>464</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "      <td>i dont know although id sure like to talk to h...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9552</td>\n",
       "      <td>32</td>\n",
       "      <td>212</td>\n",
       "      <td>Lisa Simpson: That life is worth living.</td>\n",
       "      <td>864000</td>\n",
       "      <td>true</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>That life is worth living.</td>\n",
       "      <td>that life is worth living</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9553</td>\n",
       "      <td>32</td>\n",
       "      <td>213</td>\n",
       "      <td>Edna Krabappel-Flanders: The polls will be ope...</td>\n",
       "      <td>864000</td>\n",
       "      <td>true</td>\n",
       "      <td>40</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "      <td>the polls will be open from now until the end ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9554</td>\n",
       "      <td>32</td>\n",
       "      <td>214</td>\n",
       "      <td>Martin Prince: (HOARSE WHISPER) I don't think ...</td>\n",
       "      <td>877000</td>\n",
       "      <td>true</td>\n",
       "      <td>38</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Martin Prince</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>I don't think there's anything left to say.</td>\n",
       "      <td>i dont think theres anything left to say</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9555</td>\n",
       "      <td>32</td>\n",
       "      <td>215</td>\n",
       "      <td>Edna Krabappel-Flanders: Bart?</td>\n",
       "      <td>881000</td>\n",
       "      <td>true</td>\n",
       "      <td>40</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>Bart?</td>\n",
       "      <td>bart</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9556</td>\n",
       "      <td>32</td>\n",
       "      <td>216</td>\n",
       "      <td>Bart Simpson: Victory party under the slide!</td>\n",
       "      <td>882000</td>\n",
       "      <td>true</td>\n",
       "      <td>8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>Victory party under the slide!</td>\n",
       "      <td>victory party under the slide</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9557</td>\n",
       "      <td>32</td>\n",
       "      <td>217</td>\n",
       "      <td>(Apartment Building: Ext. apartment building -...</td>\n",
       "      <td>889000</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "      <td>374.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apartment Building</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9558</td>\n",
       "      <td>32</td>\n",
       "      <td>218</td>\n",
       "      <td>Lisa Simpson: (CALLING) Mr. Bergstrom! Mr. Ber...</td>\n",
       "      <td>889000</td>\n",
       "      <td>true</td>\n",
       "      <td>9</td>\n",
       "      <td>374.0</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Apartment Building</td>\n",
       "      <td>Mr. Bergstrom! Mr. Bergstrom!</td>\n",
       "      <td>mr bergstrom mr bergstrom</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  episode_id  number  \\\n",
       "0  9549          32     209   \n",
       "1  9550          32     210   \n",
       "2  9551          32     211   \n",
       "3  9552          32     212   \n",
       "4  9553          32     213   \n",
       "5  9554          32     214   \n",
       "6  9555          32     215   \n",
       "7  9556          32     216   \n",
       "8  9557          32     217   \n",
       "9  9558          32     218   \n",
       "\n",
       "                                            raw_text timestamp_in_ms  \\\n",
       "0  Miss Hoover: No, actually, it was a little of ...          848000   \n",
       "1  Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?          856000   \n",
       "2  Miss Hoover: I don't know. Although I'd sure l...          856000   \n",
       "3           Lisa Simpson: That life is worth living.          864000   \n",
       "4  Edna Krabappel-Flanders: The polls will be ope...          864000   \n",
       "5  Martin Prince: (HOARSE WHISPER) I don't think ...          877000   \n",
       "6                     Edna Krabappel-Flanders: Bart?          881000   \n",
       "7       Bart Simpson: Victory party under the slide!          882000   \n",
       "8  (Apartment Building: Ext. apartment building -...          889000   \n",
       "9  Lisa Simpson: (CALLING) Mr. Bergstrom! Mr. Ber...          889000   \n",
       "\n",
       "  speaking_line character_id  location_id       raw_character_text  \\\n",
       "0          true          464          3.0              Miss Hoover   \n",
       "1          true            9          3.0             Lisa Simpson   \n",
       "2          true          464          3.0              Miss Hoover   \n",
       "3          true            9          3.0             Lisa Simpson   \n",
       "4          true           40          3.0  Edna Krabappel-Flanders   \n",
       "5          true           38          3.0            Martin Prince   \n",
       "6          true           40          3.0  Edna Krabappel-Flanders   \n",
       "7          true            8          3.0             Bart Simpson   \n",
       "8         false          NaN        374.0                      NaN   \n",
       "9          true            9        374.0             Lisa Simpson   \n",
       "\n",
       "               raw_location_text  \\\n",
       "0  Springfield Elementary School   \n",
       "1  Springfield Elementary School   \n",
       "2  Springfield Elementary School   \n",
       "3  Springfield Elementary School   \n",
       "4  Springfield Elementary School   \n",
       "5  Springfield Elementary School   \n",
       "6  Springfield Elementary School   \n",
       "7  Springfield Elementary School   \n",
       "8             Apartment Building   \n",
       "9             Apartment Building   \n",
       "\n",
       "                                        spoken_words  \\\n",
       "0  No, actually, it was a little of both. Sometim...   \n",
       "1                             Where's Mr. Bergstrom?   \n",
       "2  I don't know. Although I'd sure like to talk t...   \n",
       "3                         That life is worth living.   \n",
       "4  The polls will be open from now until the end ...   \n",
       "5        I don't think there's anything left to say.   \n",
       "6                                              Bart?   \n",
       "7                     Victory party under the slide!   \n",
       "8                                                NaN   \n",
       "9                      Mr. Bergstrom! Mr. Bergstrom!   \n",
       "\n",
       "                                     normalized_text word_count  \n",
       "0  no actually it was a little of both sometimes ...         31  \n",
       "1                                wheres mr bergstrom          3  \n",
       "2  i dont know although id sure like to talk to h...         22  \n",
       "3                          that life is worth living          5  \n",
       "4  the polls will be open from now until the end ...         33  \n",
       "5           i dont think theres anything left to say          8  \n",
       "6                                               bart          1  \n",
       "7                      victory party under the slide          5  \n",
       "8                                                NaN        NaN  \n",
       "9                          mr bergstrom mr bergstrom          4  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('~/Downloads/simpsons_script_lines.csv', low_memory=False)\n",
    "print(df.dtypes)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c8160cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no actually it was a little of both sometimes when a disease is in all the magazines and all the news shows its only natural that you think you have it',\n",
       " 'wheres mr bergstrom',\n",
       " 'i dont know although id sure like to talk to him he didnt touch my lesson plan what did he teach you',\n",
       " 'that life is worth living',\n",
       " 'the polls will be open from now until the end of recess now just in case any of you have decided to put any thought into this well have our final statements martin',\n",
       " 'i dont think theres anything left to say',\n",
       " 'bart',\n",
       " 'victory party under the slide',\n",
       " nan,\n",
       " 'mr bergstrom mr bergstrom']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = df['normalized_text'].tolist()\n",
    "phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82560487",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [[c for c in ph] for ph in phrases if type(ph) is str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93705591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHARS = set('abcdefghijklmnopqrstuvwxyz ')  # все символы, которые мы хотим использовать для кодировки = наш словарь\n",
    "INDEX_TO_CHAR = ['none'] + [w for w in CHARS]  # все неизвестные символы будут получать тег none\n",
    "CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}  # словарь токен-индекс\n",
    "len(INDEX_TO_CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "456dab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50  # мы хотим ограничить максимальную длину ввода\n",
    "X = torch.zeros((len(text), MAX_LEN), dtype=int)  # создаём пустой вектор для текста, чтобы класть в него индексы токенов\n",
    "for i in range(len(text)):  # для каждого предложения\n",
    "    for j, w in enumerate(text[i]):  # для каждого токена\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        X[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79fbf9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([132087, 50])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f097ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50, 28])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = nn.Embedding(len(INDEX_TO_CHAR), 28)  # размер словаря * размер вектора для кодировки каждого слова\n",
    "t = embeddings(X[0:5])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac63fbc0",
   "metadata": {},
   "source": [
    "# **Реализация сети с RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8605283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomRNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_xh = nn.Linear(input_size, hidden_size)\n",
    "        self.W_hh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        h_t = self.tanh(self.W_xh(x) + self.W_hh(h_prev))\n",
    "        return h_t\n",
    "\n",
    "\n",
    "class CustomRNNNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(CustomRNNNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn_cell = CustomRNNCell(embedding_dim, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, sentences, state=None):\n",
    "        x = self.embedding(sentences)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        if state is None:\n",
    "            state = torch.zeros(batch_size, self.hidden_size)\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            state = self.rnn_cell(x[:, t, :], state)\n",
    "            outputs.append(state)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return self.out(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8385f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(INDEX_TO_CHAR)\n",
    "embedding_dim = 30\n",
    "hidden_size = 128\n",
    "\n",
    "model = CustomRNNNetwork(vocab_size, embedding_dim, hidden_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e87c86f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 17.751, Train loss: 1.671\n",
      "Epoch 1. Time: 17.618, Train loss: 1.491\n",
      "Epoch 2. Time: 17.767, Train loss: 1.418\n",
      "Epoch 3. Time: 17.631, Train loss: 1.369\n",
      "Epoch 4. Time: 17.474, Train loss: 1.333\n",
      "Epoch 5. Time: 17.508, Train loss: 1.304\n",
      "Epoch 6. Time: 17.483, Train loss: 1.281\n",
      "Epoch 7. Time: 17.626, Train loss: 1.262\n",
      "Epoch 8. Time: 18.001, Train loss: 1.245\n",
      "Epoch 9. Time: 18.532, Train loss: 1.231\n",
      "Epoch 10. Time: 18.507, Train loss: 1.218\n",
      "Epoch 11. Time: 17.780, Train loss: 1.207\n",
      "Epoch 12. Time: 17.714, Train loss: 1.198\n",
      "Epoch 13. Time: 17.951, Train loss: 1.189\n",
      "Epoch 14. Time: 17.628, Train loss: 1.181\n",
      "Epoch 15. Time: 17.390, Train loss: 1.174\n",
      "Epoch 16. Time: 17.936, Train loss: 1.168\n",
      "Epoch 17. Time: 17.627, Train loss: 1.162\n",
      "Epoch 18. Time: 17.415, Train loss: 1.156\n",
      "Epoch 19. Time: 17.429, Train loss: 1.151\n"
     ]
    }
   ],
   "source": [
    "for ep in range(20):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    for i in range(int(len(X) / 100)):\n",
    "        \n",
    "        batch = X[i * 100:(i + 1) * 100]\n",
    "        X_batch = batch[:, :-1]\n",
    "        Y_batch = batch[:, 1:].flatten()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        answers = model.forward(X_batch)\n",
    "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
    "        loss = criterion(answers, Y_batch)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a08a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(word):\n",
    "    sentence = list(word.lower())\n",
    "    sentence = [CHAR_TO_INDEX.get(s, 0) for s in sentence]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(sentence, dtype=torch.long).unsqueeze(0)\n",
    "        answers = model(inputs)\n",
    "        probas, indices = answers.topk(1)\n",
    "        return ''.join([INDEX_TO_CHAR[ind.item()] for ind in indices.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bee60c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_TO_INDEX['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f2ff4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ompson'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('simpso')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
