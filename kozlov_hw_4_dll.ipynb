{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3504629d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bac7c",
   "metadata": {},
   "source": [
    "# **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e806f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt(text, shift, alphabet):\n",
    "    new_alphabet = alphabet[shift % len(alphabet):] + alphabet[:shift % len(alphabet)]\n",
    "    result = ''\n",
    "    for char in text:\n",
    "        if char in alphabet:\n",
    "            result += new_alphabet[alphabet.index(char)]\n",
    "        else:\n",
    "            result += char\n",
    "    return result\n",
    "\n",
    "def decrypt(text, shift, alphabet):\n",
    "    return encrypt(text, -shift, alphabet)\n",
    "\n",
    "def generate_data(alphabet, max_len=10, shift=3):\n",
    "    length = random.randint(5, max_len)\n",
    "    text = ''.join(random.choice(alphabet) for _ in range(length))\n",
    "    encrypted = encrypt(text, shift, alphabet=alphabet)\n",
    "    return {'text': text, 'encrypted': encrypted, 'length': length}\n",
    "\n",
    "def text_to_one_hot(text, alphabet):\n",
    "    seq_len = len(text)\n",
    "    one_hot = np.zeros((seq_len, len(alphabet)), dtype=np.float32)\n",
    "    for i, char in enumerate(text):\n",
    "        if char in alphabet:\n",
    "            one_hot[i, alphabet.index(char)] = 1\n",
    "    return torch.tensor(one_hot)\n",
    "\n",
    "def dataset_to_tensors(dataset, alphabet):\n",
    "    max_len = max(pair['length'] for pair in dataset)\n",
    "    X = torch.zeros((len(dataset), max_len, len(alphabet)), dtype=torch.float32)\n",
    "    y = torch.zeros((len(dataset), max_len, len(alphabet)), dtype=torch.float32)\n",
    "    \n",
    "    for i, pair in enumerate(dataset):\n",
    "        X[i, :len(pair['encrypted'])] = text_to_one_hot(pair['encrypted'], alphabet)\n",
    "        y[i, :len(pair['text'])] = text_to_one_hot(pair['text'], alphabet)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "class CaesarNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CaesarNet, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        o, _ = self.rnn(sentence)\n",
    "        return self.linear(o)\n",
    "\n",
    "    def train_model(self, train_loader, alphabet_size, lr, epochs=10):\n",
    "        self.to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        start = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            train_loss, train_acc, train_iter_num = 0., 0., 0.\n",
    "            \n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = self(X_batch)\n",
    "                y_indices = torch.argmax(y_batch, dim=-1)\n",
    "                \n",
    "                loss = criterion(output.view(-1, alphabet_size), y_indices.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                batch_acc = (output.argmax(dim=-1) == y_indices).float().mean().item()\n",
    "                train_acc += batch_acc\n",
    "                train_iter_num += 1\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1}, loss: {train_loss/train_iter_num:.4f}, acc: \"\n",
    "                f\"{train_acc/train_iter_num:.4f}, \"\n",
    "                f\"{time.time() - start:.2f} sec.\"\n",
    "            )\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, text, alphabet):\n",
    "        self.eval()\n",
    "        best_shift = 0\n",
    "        best_score = -1.\n",
    "        best_pred_text = \"\"\n",
    "        \n",
    "        for shift in range(len(alphabet)):\n",
    "            encrypted = encrypt(text, shift, alphabet)\n",
    "            X_test = text_to_one_hot(encrypted, alphabet)[None, :]\n",
    "            with torch.no_grad():\n",
    "                X_test = X_test.to(device)\n",
    "                pred = self(X_test)\n",
    "            pred_text = ''.join(alphabet[torch.argmax(char).item()] for char in pred[0][:len(text)])\n",
    "            \n",
    "            matches = sum(1 for c1, c2 in zip(text, pred_text) if c1 == c2)\n",
    "            score = matches / len(text) if len(text) > 0 else 0.\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_shift = shift\n",
    "                best_pred_text = pred_text\n",
    "        \n",
    "        encrypted = encrypt(text, best_shift, alphabet)\n",
    "        return encrypted, best_pred_text, best_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56779ce6",
   "metadata": {},
   "source": [
    "# **Creation Alphabet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d7702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz' + 'abcdefghijklmnopqrstuvwxyz'.upper() + 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя' + 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'.upper() + r'0123456789,. ?|/\\[]()\";:'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86d45f",
   "metadata": {},
   "source": [
    "# **Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3191ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_size = len(alphabet)\n",
    "input_size = output_size = alphabet_size\n",
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "learning_rate = .001\n",
    "num_epochs = 10\n",
    "shift = random.randint(-3555456654575, 3555456654575) % alphabet_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38026235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cdf9e4",
   "metadata": {},
   "source": [
    "# **Creation initial dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df9456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_init = [generate_data(alphabet, max_len=10, shift=shift) for _ in range(10000)]\n",
    "X, y = dataset_to_tensors(dataset_init, alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69a26395",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, y)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01b25e",
   "metadata": {},
   "source": [
    "# **Creation Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb670663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaesarNet(\n",
       "  (rnn): RNN(142, 128, batch_first=True)\n",
       "  (linear): Linear(in_features=128, out_features=142, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CaesarNet(input_size, hidden_size, output_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df106771",
   "metadata": {},
   "source": [
    "# **Training model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "418cc9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, loss: 0.0201, acc: 1.0000, 4.46 sec.\n"
     ]
    }
   ],
   "source": [
    "model = model.train_model(train_loader, alphabet_size, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50743b76",
   "metadata": {},
   "source": [
    "# **Experiment 1: with bruteforce shift**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864938ac",
   "metadata": {},
   "source": [
    "## **Testing Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae5062be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Привет мир, это я DJ 23\n",
      "Encrypted: VphadrчlhpхчCrnчEчЯ5чно\n",
      "Predicted: Привет мир, это я DJ 23\n",
      "Learning Shift: 88\n",
      "Initial Shift: 88\n"
     ]
    }
   ],
   "source": [
    "text = \"Привет мир, это я DJ 23\"\n",
    "encrypted, predicted, learning_shift = model.predict(text, alphabet)\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Encrypted: {encrypted}\")\n",
    "print(f\"Predicted: {predicted}\")\n",
    "print(f\"Learning Shift: {learning_shift}\")\n",
    "print(f\"Initial Shift: {shift}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3efc3451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: cV8бVG\n",
      "Encrypted: Е[у:[2\n",
      "Predicted: cV8бVG\n",
      "Learning Shift: 88\n",
      "Initial Shift: 88\n"
     ]
    }
   ],
   "source": [
    "text = dataset_init[9999]['text']\n",
    "encrypted, predicted, learning_shift = model.predict(text, alphabet)\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Encrypted: {encrypted}\")\n",
    "print(f\"Predicted: {predicted}\")\n",
    "print(f\"Learning Shift: {learning_shift}\")\n",
    "print(f\"Initial Shift: {shift}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a4190",
   "metadata": {},
   "source": [
    "# **Experiment 2: with trainable shift**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce05d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaesarNetTrainShift(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CaesarNetTrainShift, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.shift = nn.Parameter(torch.tensor(float(random.randint(0, input_size - 1))))\n",
    "        self.alphabet_size = input_size\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        normalized_shift = torch.sigmoid(self.shift) * self.alphabet_size\n",
    "        shift = torch.round(normalized_shift).long() % self.alphabet_size\n",
    "        shifted_sentence = torch.roll(sentence, shifts=shift.item(), dims=-1)\n",
    "        o, _ = self.rnn(shifted_sentence)\n",
    "        return self.linear(o)\n",
    "\n",
    "    def train_model(self, train_loader, alphabet_size, lr, epochs=10):\n",
    "        self.to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam([\n",
    "        {'params': [self.shift], 'lr': .01},\n",
    "        {'params': [p for p in self.parameters() if p is not self.shift], 'lr': lr}\n",
    "    ], weight_decay=1e-4)\n",
    "        start = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            train_loss, train_acc, train_iter_num = 0., 0., 0.\n",
    "            \n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = self(X_batch)\n",
    "                y_indices = torch.argmax(y_batch, dim=-1)\n",
    "                \n",
    "                loss = criterion(output.view(-1, alphabet_size), y_indices.view(-1)) + .01 * torch.abs(self.shift)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                batch_acc = (output.argmax(dim=-1) == y_indices).float().mean().item()\n",
    "                train_acc += batch_acc\n",
    "                train_iter_num += 1\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1}, loss: {train_loss/train_iter_num:.4f}, acc: \"\n",
    "                f\"{train_acc/train_iter_num:.4f},\"\n",
    "                f\"{time.time() - start:.2f} sec.\"\n",
    "            )\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, text, alphabet):\n",
    "        self.eval()\n",
    "\n",
    "        normalized_shift = torch.sigmoid(self.shift) * len(alphabet)\n",
    "        shift = int(round(normalized_shift.item())) % len(alphabet)\n",
    "        encrypted = encrypt(text=text, shift=shift, alphabet=alphabet)\n",
    "        X_test = text_to_one_hot(encrypted, alphabet)[None, :]\n",
    "        with torch.no_grad():\n",
    "            X_test = X_test.to(device)\n",
    "            pred = self(X_test)\n",
    "        pred_text = ''.join(alphabet[torch.argmax(char).item()] for char in pred[0])\n",
    "        return encrypted, pred_text, shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aacd7d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaesarNetTrainShift(\n",
       "  (rnn): RNN(142, 128, batch_first=True)\n",
       "  (linear): Linear(in_features=128, out_features=142, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = CaesarNetTrainShift(input_size, hidden_size, output_size)\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6723fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, loss: 0.0202, acc: 1.0000,5.10 sec.\n"
     ]
    }
   ],
   "source": [
    "model1 = model1.train_model(train_loader, alphabet_size, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1866824a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(0.0005, device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b53f5040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Привет мир, это я DJ 23\n",
      "Encrypted: E;|7,aж]|;ежla)жnжОФжXY\n",
      "Predicted: яаSLOвЫWSаЩЫмвYЫоЫmsЫСТ\n",
      "Learning Shift: 71\n",
      "Initial Shift: 88\n"
     ]
    }
   ],
   "source": [
    "text = \"Привет мир, это я DJ 23\"\n",
    "encrypted, predicted, learning_shift = model1.predict(text, alphabet)\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Encrypted: {encrypted}\")\n",
    "print(f\"Predicted: {predicted}\")\n",
    "print(f\"Learning Shift: {learning_shift}\")\n",
    "print(f\"Initial Shift: {shift}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
